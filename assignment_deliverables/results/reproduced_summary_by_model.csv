model,n,mean_persuasion,mean_log_param
Llama-2-13b-hf,819,57.9988,23.2882
Llama-2-70b-hf,2427,56.8156,24.9718
Llama-2-7b-hf,355,56.7669,22.6692
Qwen1.5-0.5B,350,48.8936,20.0301
Qwen1.5-1.8B,343,54.1698,21.3111
Qwen1.5-14B,819,56.0003,23.3623
Qwen1.5-4B,329,55.8898,22.1096
Qwen1.5-72B,2447,58.3016,24.9999
Qwen1.5-7B,362,58.0325,22.6692
Yi-34B,786,59.1174,24.2496
Yi-6B,348,54.954,22.515
Yi-9B,807,56.5468,22.9205
falcon-40b,817,56.3586,24.4121
falcon-7b,361,55.4529,22.6692
pythia-1.4b,346,53.7695,21.0597
pythia-12b,817,53.0704,23.2082
pythia-160m,382,48.4856,18.8907
pythia-1b,348,51.6559,20.7233
pythia-2.8b,327,53.4625,21.7529
pythia-410m,367,49.6076,19.8317
pythia-6.9b,329,54.1991,22.6548
pythia-70m,360,49.2,18.064
